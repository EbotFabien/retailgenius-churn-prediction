{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetailGenius Customer Churn Prediction\n",
    "\n",
    "## EPITA - AI Project Methodology 2025-2026\n",
    "\n",
    "This notebook provides an end-to-end exploration and implementation of the customer churn prediction project.(No need to run since we have ran Makefile)\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Imports](#1.-Setup-and-Imports)\n",
    "2. [Data Loading and Exploration](#2.-Data-Loading-and-Exploration)\n",
    "3. [Data Preparation](#3.-Data-Preparation)\n",
    "4. [Feature Engineering](#4.-Feature-Engineering)\n",
    "5. [Model Training with MLflow](#5.-Model-Training-with-MLflow)\n",
    "6. [Model Evaluation](#6.-Model-Evaluation)\n",
    "7. [SHAP Analysis (Explainable AI)](#7.-SHAP-Analysis)\n",
    "8. [Inference](#8.-Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ebotfabien/Desktop/AI_Project_Methodology/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Python version: 3.9.6 (default, Feb  3 2024, 15:58:27) \n",
      "[Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "MLflow version: 3.1.4\n",
      "SHAP version: 0.49.1\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report, roc_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21, 4)\n",
      "\n",
      "Columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "DATA_PATH = '../data/raw/E Commerce Dataset.xlsx'\n",
    "\n",
    "# Try different file formats\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "except:\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw/ecommerce_churn.csv')\n",
    "    except:\n",
    "        print(\"Please place your dataset in data/raw/ directory\")\n",
    "        raise\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data</td>\n",
       "      <td>Variable</td>\n",
       "      <td>Discerption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>E Comm</td>\n",
       "      <td>CustomerID</td>\n",
       "      <td>Unique customer ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>E Comm</td>\n",
       "      <td>Churn</td>\n",
       "      <td>Churn Flag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>E Comm</td>\n",
       "      <td>Tenure</td>\n",
       "      <td>Tenure of customer in organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>E Comm</td>\n",
       "      <td>PreferredLoginDevice</td>\n",
       "      <td>Preferred login device of customer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 1            Unnamed: 2  \\\n",
       "0         NaN       Data              Variable   \n",
       "1         NaN     E Comm            CustomerID   \n",
       "2         NaN     E Comm                 Churn   \n",
       "3         NaN     E Comm                Tenure   \n",
       "4         NaN     E Comm  PreferredLoginDevice   \n",
       "\n",
       "                           Unnamed: 3  \n",
       "0                         Discerption  \n",
       "1                  Unique customer ID  \n",
       "2                          Churn Flag  \n",
       "3  Tenure of customer in organization  \n",
       "4  Preferred login device of customer  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  0 non-null      float64\n",
      " 1   Unnamed: 1  21 non-null     object \n",
      " 2   Unnamed: 2  21 non-null     object \n",
      " 3   Unnamed: 3  21 non-null     object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='Churn', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Churn Distribution (Count)')\n",
    "\n",
    "# Pie chart\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "axes[1].pie(churn_counts.values, labels=['No Churn', 'Churn'], \n",
    "            autopct='%1.1f%%', colors=['#66b3ff', '#ff9999'])\n",
    "axes[1].set_title('Churn Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/churn_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories\n",
    "NUMERICAL_FEATURES = [\n",
    "    'Tenure', 'WarehouseToHome', 'HourSpendOnApp',\n",
    "    'NumberOfDeviceRegistered', 'SatisfactionScore', 'NumberOfAddress',\n",
    "    'Complain', 'OrderAmountHikeFromlastYear', 'CouponUsed',\n",
    "    'OrderCount', 'DaySinceLastOrder', 'CashbackAmount'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'PreferredLoginDevice', 'CityTier', 'PreferredPaymentMode',\n",
    "    'Gender', 'PreferedOrderCat', 'MaritalStatus'\n",
    "]\n",
    "\n",
    "TARGET = 'Churn'\n",
    "ID_COLUMN = 'CustomerID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"Removed {initial_rows - len(df_clean)} duplicate rows\")\n",
    "\n",
    "# Handle missing values in numerical columns\n",
    "for col in NUMERICAL_FEATURES:\n",
    "    if col in df_clean.columns:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"Filled {missing} missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "# Handle missing values in categorical columns\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_clean.columns:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean[col].mode()[0]\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "            print(f\"Filled {missing} missing values in '{col}' with mode: {mode_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "df_clean.to_csv('../data/interim/cleaned_data.csv', index=False)\n",
    "print(f\"Saved cleaned data: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "# Average cashback per order\n",
    "df_features['AvgCashbackPerOrder'] = df_features['CashbackAmount'] / df_features['OrderCount'].replace(0, 1)\n",
    "\n",
    "# Engagement score\n",
    "df_features['EngagementScore'] = df_features['HourSpendOnApp'] * df_features['NumberOfDeviceRegistered']\n",
    "\n",
    "# Activity level\n",
    "df_features['ActivityLevel'] = df_features['OrderCount'] / df_features['DaySinceLastOrder'].replace(0, 1)\n",
    "\n",
    "# Coupon usage rate\n",
    "df_features['CouponUsageRate'] = df_features['CouponUsed'] / df_features['OrderCount'].replace(0, 1)\n",
    "\n",
    "print(\"Created derived features:\")\n",
    "print(\"- AvgCashbackPerOrder\")\n",
    "print(\"- EngagementScore\")\n",
    "print(\"- ActivityLevel\")\n",
    "print(\"- CouponUsageRate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "encoders = {}\n",
    "\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_features.columns:\n",
    "        encoder = LabelEncoder()\n",
    "        df_features[col] = df_features[col].astype(str)\n",
    "        df_features[col] = encoder.fit_transform(df_features[col])\n",
    "        encoders[col] = encoder\n",
    "        print(f\"Encoded '{col}' with {len(encoder.classes_)} classes\")\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(encoders, '../models/encoders.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [col for col in df_features.columns if col not in [TARGET, ID_COLUMN]]\n",
    "X = df_features[feature_cols]\n",
    "y = df_features[TARGET]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nChurn rate - Train: {y_train.mean():.2%}, Test: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get all numerical columns (original + derived)\n",
    "numerical_cols = NUMERICAL_FEATURES + ['AvgCashbackPerOrder', 'EngagementScore', 'ActivityLevel', 'CouponUsageRate']\n",
    "numerical_cols = [col for col in numerical_cols if col in X_train.columns]\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "X_train_scaled.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save feature names\n",
    "with open('../data/processed/feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(X_train_scaled.columns.tolist()))\n",
    "\n",
    "print(\"Saved processed data to data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri('mlruns')\n",
    "mlflow.set_experiment('RetailGenius_Churn_Prediction')\n",
    "\n",
    "print(\"MLflow experiment set up successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    return metrics, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, min_samples_split=5,\n",
    "            min_samples_leaf=2, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE,\n",
    "            use_label_encoder=False, eval_metric='logloss'\n",
    "        ),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model': lgb.LGBMClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            num_leaves=31, random_state=RANDOM_STATE, verbose=-1\n",
    "        ),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'num_leaves': 31}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with MLflow tracking\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(model_config['params'])\n",
    "        \n",
    "        # Train model\n",
    "        model = model_config['model']\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics, y_pred, y_pred_proba = evaluate_model(model, X_test_scaled, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "            print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        # Log model\n",
    "        if model_name == 'random_forest':\n",
    "            mlflow.sklearn.log_model(model, 'model')\n",
    "        elif model_name == 'xgboost':\n",
    "            mlflow.xgboost.log_model(model, 'model')\n",
    "        elif model_name == 'lightgbm':\n",
    "            mlflow.lightgbm.log_model(model, 'model')\n",
    "        \n",
    "        # Save model locally\n",
    "        joblib.dump(model, f'../models/{model_name}_model.joblib')\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        # Track best model\n",
    "        if metrics['f1_score'] > best_score:\n",
    "            best_score = metrics['f1_score']\n",
    "            best_model = model\n",
    "            best_model_name = model_name\n",
    "\n",
    "print(f\"\\n\\nBest Model: {best_model_name} (F1 Score: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "joblib.dump(best_model, '../models/best_model.joblib')\n",
    "print(f\"Best model saved to models/best_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "best_results = results[best_model_name]\n",
    "cm = confusion_matrix(y_test, best_results['predictions'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.savefig('../reports/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "    auc_score = result['metrics']['roc_auc']\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.savefig('../reports/figures/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train_scaled.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True).tail(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'], color='steelblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.savefig('../reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Analysis (Explainable AI)\n",
    "\n",
    "### Part 3 of the Graded Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "shap.initjs()\n",
    "\n",
    "# Create TreeExplainer\n",
    "print(\"Creating SHAP TreeExplainer...\")\n",
    "X_background = X_train_scaled.sample(n=100, random_state=RANDOM_STATE)\n",
    "explainer = shap.TreeExplainer(best_model, X_background)\n",
    "print(\"Explainer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "print(\"Computing SHAP values...\")\n",
    "X_sample = X_test_scaled.sample(n=min(500, len(X_test_scaled)), random_state=RANDOM_STATE)\n",
    "shap_values = explainer(X_sample)\n",
    "print(f\"SHAP values computed for {len(X_sample)} samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for a specific sample\n",
    "sample_idx = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if len(shap_values.shape) == 3:\n",
    "    shap.plots.waterfall(shap_values[sample_idx, :, 1], show=False)\n",
    "else:\n",
    "    shap.plots.waterfall(shap_values[sample_idx], show=False)\n",
    "plt.title(f'SHAP Waterfall Plot - Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../reports/figures/shap_waterfall_sample_{sample_idx}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (dot)\n",
    "plt.figure(figsize=(12, 8))\n",
    "if len(shap_values.shape) == 3:\n",
    "    shap.summary_plot(shap_values.values[:, :, 1], X_sample, show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values.values, X_sample, show=False)\n",
    "plt.title('SHAP Summary Plot')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/shap_summary_dot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean SHAP values (bar plot)\n",
    "plt.figure(figsize=(12, 8))\n",
    "if len(shap_values.shape) == 3:\n",
    "    shap.summary_plot(shap_values.values[:, :, 1], X_sample, plot_type='bar', show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values.values, X_sample, plot_type='bar', show=False)\n",
    "plt.title('Mean Absolute SHAP Values')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/shap_mean_values.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "if len(shap_values.shape) == 3:\n",
    "    shap.plots.beeswarm(shap_values[:, :, 1], show=False)\n",
    "else:\n",
    "    shap.plots.beeswarm(shap_values, show=False)\n",
    "plt.title('SHAP Beeswarm Plot')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/shap_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot for a specific sample\n",
    "sample_idx = 0\n",
    "\n",
    "if hasattr(explainer, 'expected_value'):\n",
    "    if isinstance(explainer.expected_value, np.ndarray):\n",
    "        expected_value = explainer.expected_value[1]\n",
    "    else:\n",
    "        expected_value = explainer.expected_value\n",
    "else:\n",
    "    expected_value = shap_values.base_values[sample_idx]\n",
    "\n",
    "if len(shap_values.shape) == 3:\n",
    "    sample_shap = shap_values.values[sample_idx, :, 1]\n",
    "else:\n",
    "    sample_shap = shap_values.values[sample_idx]\n",
    "\n",
    "shap.force_plot(expected_value, sample_shap, X_sample.iloc[sample_idx], matplotlib=True, show=False)\n",
    "plt.title(f'SHAP Force Plot - Sample {sample_idx}')\n",
    "plt.savefig(f'../reports/figures/shap_force_sample_{sample_idx}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots for top features\n",
    "if len(shap_values.shape) == 3:\n",
    "    mean_abs_shap = np.abs(shap_values.values[:, :, 1]).mean(axis=0)\n",
    "else:\n",
    "    mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "top_features_idx = np.argsort(mean_abs_shap)[-3:][::-1]\n",
    "top_features = [X_sample.columns[i] for i in top_features_idx]\n",
    "\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if len(shap_values.shape) == 3:\n",
    "        shap.dependence_plot(feature, shap_values.values[:, :, 1], X_sample, show=False)\n",
    "    else:\n",
    "        shap.dependence_plot(feature, shap_values.values, X_sample, show=False)\n",
    "    plt.title(f'SHAP Dependence Plot - {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/shap_dependence_{feature}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SHAP feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "feature_importance.to_csv('../reports/figures/shap_feature_importance.csv', index=False)\n",
    "print(\"\\nTop 10 Features by SHAP Importance:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "model = joblib.load('../models/best_model.joblib')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create results DataFrame\n",
    "inference_results = pd.DataFrame({\n",
    "    'prediction': predictions,\n",
    "    'churn_probability': probabilities,\n",
    "    'risk_level': pd.cut(probabilities, bins=[0, 0.3, 0.6, 1.0], labels=['Low', 'Medium', 'High'])\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "inference_results.to_csv('../data/processed/predictions.csv', index=False)\n",
    "\n",
    "print(\"\\nInference Summary:\")\n",
    "print(f\"Total samples: {len(inference_results)}\")\n",
    "print(f\"Predicted churners: {(predictions == 1).sum()}\")\n",
    "print(f\"Predicted non-churners: {(predictions == 0).sum()}\")\n",
    "print(f\"\\nRisk Level Distribution:\")\n",
    "print(inference_results['risk_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Data Preparation**: Loading, cleaning, and preprocessing the E-Commerce churn dataset\n",
    "2. **Feature Engineering**: Creating derived features and encoding categorical variables\n",
    "3. **Model Training**: Training Random Forest, XGBoost, and LightGBM with MLflow tracking\n",
    "4. **Model Evaluation**: Confusion matrix, ROC curves, and feature importance\n",
    "5. **SHAP Analysis**: Comprehensive explainability with waterfall, force, summary, beeswarm, and dependence plots\n",
    "6. **Inference**: Making predictions and risk assessment\n",
    "\n",
    "### Next Steps\n",
    "- Run `mlflow ui` to view experiment results\n",
    "- Deploy the model using `mlflow models serve`\n",
    "- Check the `reports/figures/` directory for all generated visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
